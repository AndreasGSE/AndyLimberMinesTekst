{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to think about\n",
    "- Removing short reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup # for maniuplating html\n",
    "\n",
    "# START OF REVIEW EXTRACTING\n",
    "\n",
    "with open('car_output.txt', 'r') as stringfile: # so we load in ALL the data as a massive string\n",
    "\tcar_output = stringfile.read()\n",
    "\n",
    "out = [\"<!DOCTYPE html>\" + rest for rest in car_output.split(\"<!DOCTYPE html>\")] # split up by start pf page, but add the start point\n",
    "\n",
    "out_soup = [BeautifulSoup(html, \"html.parser\") for html in out[1:]] # convert to beautiful soup\n",
    "\n",
    "# I think the above is standard, now we actually do our document (i.e. if change URLs to look at)\n",
    "\n",
    "# Now extracting reviews - golf gives me 2098, could add another car to get more\n",
    "review_rating = []\n",
    "for page in out_soup:\n",
    "\tpage_contents = page.find_all(\"div\", itemprop=\"review\") # locating all reviews on a page\n",
    "\tresult = [(review.find(itemprop=\"reviewBody\").get_text().encode(\"ascii\",\"replace\"), # getting text\n",
    "\t           review.find(class_=\"size8\").get_text().encode(\"ascii\",\"replace\").split()[0]) for review in page_contents] # getting rating\n",
    "\t[review_rating.append(review) for review in result]\n",
    "\n",
    "# Removing empty reviews and getting rid of paragraphs and saving review and rating as a tuple\n",
    "review_rating_red = [(review[0].replace('\\n', ' ').replace('\\r', ''), review[1]) for review in review_rating if review[0] != \"Reviewer left no comment\"] # leaves us with 151 reviews\n",
    "\n",
    "# Save all data just for safety\n",
    "with open(\"reviews_ratings.txt\", \"w\") as out_file:\n",
    "\tout_file.write(\"\\n\".join(\"%s %s\" % review for review in review_rating_red))\n",
    "    \n",
    "# Getting reviews\n",
    "reviews = [review[0] for review in review_rating_red]\n",
    "reviews[1] = \"Top car I've had every convertible VW have made and still have a mk 1 golf but not cabriollet\" # Just randomly noticed this review was written by an idiot\n",
    "\n",
    "# Getting ratings\n",
    "ratings = [review[1] for review in review_rating_red] # distribution of ratings is not high, may not use\n",
    "\n",
    "# END OF REVIEW EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer # could change the stemmer or lemmatise\n",
    "\n",
    "# START OF CLASS CREATION\n",
    "# Will create a class that will get rating and text\n",
    "class Review():\n",
    "    # On initialising\n",
    "    def __init__(self, review_text, review_rating):\n",
    "        self.text = review_text.lower() # NOTE that only the tokens are fixed up, text kept the same\n",
    "        self.rating = review_rating\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text)) # Would like to split on punct as well to remove later\n",
    "    \n",
    "    # Remove non-alphanumeric\n",
    "    def token_remove_alpha(self):\n",
    "            self.tokens = np.array([word for word in self.tokens if word.isalpha()])\n",
    "    \n",
    "    # Remove stop words\n",
    "    def token_stop_remove(self, stopwords):\n",
    "            self.tokens = np.array([word for word in self.tokens if word not in stopwords])\n",
    "            \n",
    "    # Now stem everything\n",
    "    def token_stem(self):\n",
    "            self.tokens = np.array([PorterStemmer().stem(word) for word in self.tokens])\n",
    "    \n",
    "    # Define a process that will clean our tokens\n",
    "    def token_clean_up(self, stopwords):    \n",
    "        # Now cleaning up\n",
    "        self.token_remove_alpha()\n",
    "        self.token_stop_remove(stopwords)\n",
    "        self.token_stem()\n",
    "    \n",
    "# END OF CLASS CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the stop words used, also excluded some common \"mistakes\" like youll or youre\n",
    "\n",
    "Also included some words previously included like \"high\", \"first\", \"long\", as they could refer to performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# START OF CORPUS\n",
    "class Review_set():\n",
    "    # On initialising\n",
    "    def __init__(self, review_data, stopword_file):\n",
    "        self.reviews = [Review(review[0], review[1]) for review in review_data] # feed in the tuples of ratings etc.\n",
    "        self.create_stopwords(stopword_file) # stopword element\n",
    "        \n",
    "        # Now for each document we will get a cleaned up token set - note that this is done for each document\n",
    "        self.clean_reviews() \n",
    "        \n",
    "        self.tokenise_reviews() # return reviews but in tokenised form\n",
    "        \n",
    "        # Getting unique tokens in the review set - note we pull data from each document\n",
    "        self.get_unique_tokens()\n",
    "    \n",
    "    # Loading and creating stopwords\n",
    "    def create_stopwords(self, stopword_file):\n",
    "        self.stopwords = np.array(np.loadtxt(\"stopwords.txt\", dtype = str, delimiter = \"\\n\"))\n",
    "        \n",
    "    # Cleaning all documents\n",
    "    def clean_reviews(self):\n",
    "        [review.token_clean_up(self.stopwords) for review in self.reviews]\n",
    "        \n",
    "    # Getting reviews in tokenised form\n",
    "    def tokenise_reviews(self):\n",
    "        self.tokenised_reviews = [\" \".join(review.tokens) for review in self.reviews]\n",
    "        \n",
    "    # Getting the unique set of tokens\n",
    "    def get_unique_tokens(self):\n",
    "        self.unique_tokens = set()\n",
    "        for review in self.reviews:\n",
    "            self.unique_tokens = self.unique_tokens.union(review.tokens)\n",
    "        \n",
    "    # Document term matrix of CLEANED UP TOKENS - NOTE that will return in alphabetical order of words\n",
    "    def doc_term_mat(self):\n",
    "        vectorisor = CountVectorizer()\n",
    "        count_fit = vectorisor.fit_transform(self.tokenised_reviews)\n",
    "        \n",
    "        self.DTM = count_fit.toarray() # return array of counts\n",
    "    \n",
    "    # Get tf_idf scores\n",
    "    def tf_idf_score(self):   \n",
    "        vectorisor = TfidfVectorizer()\n",
    "        tf_idf_fit = vectorisor.fit_transform(self.tokenised_reviews)\n",
    "        \n",
    "        # Now return tuples of words and their associated score - note that am returning ALL\n",
    "        self.tf_idf = zip(vectorisor.get_feature_names(),vectorisor.idf_)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately will want to find common topics or \"trends\" in the reviews. To identify this could look for\n",
    "- Commonly occuring words: see common trends\n",
    "- Important words: maybe we can use this to identify more in depth reviews that focus on faults\n",
    "\n",
    "I guess then what would be left would be to try and identify the topics by looking at the common words\n",
    "\n",
    "Could extend and look at some kind of sentiment analysis of the top words in a topic\n",
    "\n",
    "Issues\n",
    "- Haven't really specified stop words\n",
    "- Superlatives etc. will really mess up the data, really want to focus on the technical aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the corpus etc\n",
    "review_data = zip(reviews, ratings)\n",
    "review_corpus = Review_set(review_data, \"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4194L, 4005L)\n",
      "1861\n",
      "913\n",
      "(4194L, 418L)\n",
      "269\n",
      "(3925L, 418L)\n"
     ]
    }
   ],
   "source": [
    "# Just looking at dimensions and reducing by some words / documents\n",
    "\n",
    "review_corpus.doc_term_mat()\n",
    "\n",
    "doc_ratio = 0.00024\n",
    "\n",
    "vectorisor = CountVectorizer(min_df = 20, max_df = 2000) # 1138\n",
    "count_fit = vectorisor.fit_transform(review_corpus.tokenised_reviews)\n",
    "\n",
    "print review_corpus.DTM.shape # OLD SHAPE\n",
    "\n",
    "print sum(np.sum(review_corpus.DTM, axis = 0) == 1) # 1334 words only have one entry\n",
    "print max(np.sum(count_fit.toarray(), axis = 0)) # no word is present in more than 1000\n",
    "\n",
    "print count_fit.toarray().shape # NEW SHAPE\n",
    "print sum(np.sum(count_fit.toarray(), axis = 1) == 0)\n",
    "\n",
    "print count_fit.toarray()[np.sum(count_fit.toarray(), axis = 1) != 0,].shape # GET RID OF ZERO WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x000000008C78B5C8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "count_fit = vectorisor.fit_transform(review_corpus.tokenised_reviews)\n",
    "DT_matrix_reduced = count_fit.toarray()[np.sum(count_fit.toarray(), axis = 1) != 0,] # contain zero words\n",
    "# This could be done in a different way whereby we delete from start any short reviews??\n",
    "\n",
    "# PARAMETERS\n",
    "n_topics = 3\n",
    "n_iter = 30000\n",
    "###\n",
    "\n",
    "lda_model = lda.LDA(n_iter = n_iter, n_topics = n_topics)\n",
    "#lda_model.fit(review_corpus.DTM)\n",
    "lda_model.fit(DT_matrix_reduced)\n",
    "\n",
    "# Running an LDA to try and uncover topics present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print [key for key in vectorisor.vocabulary_.iterkeys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: year problem mile vw new servic time reliabl replac fault engin\n",
      "Topic 1: reliabl comfort fuel economi econom excel famili run easi power engin\n",
      "Topic 2: best year reliabl vw own car buy new will recommend drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = sorted([word[0] for word in zip(vectorisor.get_feature_names(), np.asarray(count_fit.sum(axis = 0)).ravel())])\n",
    "\n",
    "topic_word = lda_model.topic_word_\n",
    "n_top_words = 12\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove things like \"love\" and general evaluation, because will do a sentiment thing on that later\n",
    "- although then we will need a different set of stop words to do this, which seems inefficient\n",
    "\n",
    "\n",
    "For the sentiment - could either do an average sentiment of each document and then average by weight\n",
    "OR could do an actual sentiment on the words in the topic\n",
    "- the only thing is that \"engine\" has no connotation, but \"has a great engine\" does, so we get by document rather than word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SENTIMENT\n",
    "## Getting AFINN\n",
    "import string\n",
    "AFINN = dict(map(lambda (k,v): (k,int(v)), [line.split('\\t') for line in open(\"./AFINN/AFINN-111.txt\")]))\n",
    "\n",
    "## NOW HOW TO RETURN SCORE - note feed a string which is the entire review - then outside we loop\n",
    "def sentiment_scores(review_string):\n",
    "    # remove punct from string we feed\n",
    "    string_punct_rem = review_string.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    # Go over each word in the string to get score\n",
    "    score_list = map(lambda word: AFINN.get(word, 0), string_punct_rem.lower().split())\n",
    "    \n",
    "    return score_list\n",
    "\n",
    "###\n",
    "\n",
    "# Feeding in each review in string form\n",
    "review_scores = [sentiment_scores(review) for review in reviews]\n",
    "non_zero = map(lambda score_list: [score for score in score_list if score != 0], review_scores) # maybe should do an \"if not None\" or something\n",
    "review_scores_sum = [sum(score) for score in review_scores]\n",
    "review_scores_mean = [np.mean(score) if len(score) != 0 else 0 for score in non_zero] # should only include those entries that are non-zero`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision of whether to use full text - or to use some tokenised form. I guess the thing is that if we use tokenised, there will be many words not included, while the AFINN is supposed to be very complete\n",
    "\n",
    "\n",
    "To proceed - we now have the sentiment score for each of the reviews (SHOULD EVALUATE A POOR SCORE ONE)\n",
    "Then what we want to do is loop over the topics and add to its score - the proportion of the score explained by the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1254.29229625  2741.11178707  2637.23851991]\n"
     ]
    }
   ],
   "source": [
    "review_scores_mean_red = np.array(review_scores_mean)[np.sum(count_fit.toarray(), axis = 1) != 0,]\n",
    "\n",
    "# FIRST TRIAL OF TOPIC SENTIMENT\n",
    "print np.dot(np.transpose(review_scores_mean_red),lda_model.doc_topic_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30650529  0.35456518  0.33892953]\n",
      "[ 0.99449036  0.98963731  0.99267399]\n",
      "[ 0.003003    0.00146413  0.00095877]\n",
      "[ 1203.03324433  1391.66834515  1330.29841053]\n"
     ]
    }
   ],
   "source": [
    "# get a look at the average probabilities\n",
    "print lda_model.doc_topic_.mean(axis = 0)\n",
    "print lda_model.doc_topic_.max(axis = 0)\n",
    "print lda_model.doc_topic_.min(axis = 0)\n",
    "print lda_model.doc_topic_.sum(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.04260818  1.96965879  1.98244131]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "doc_topic_normed = lda_model.doc_topic_ / lda_model.doc_topic_.sum(axis = 0)\n",
    "\n",
    "# Second trial with attempt to normalise\n",
    "print np.dot(np.transpose(review_scores_mean_red), doc_topic_normed)\n",
    "\n",
    "# Looks better now and at least has some sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ideas for further\n",
    "- Could do a word cloud where we have size by probability and colour by sentiment\n",
    "- Then another thing to do is to look at the relationship between frequency of words and their sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying to make a data frame where we have a word,\n",
    "# the probability for each topic, and then the sentiment\n",
    "\n",
    "# The words\n",
    "words = vocab\n",
    "\n",
    "# Their scores\n",
    "scores_1 = topic_word[0] # comes as three lists\n",
    "scores_2 = topic_word[1]\n",
    "scores_3 = topic_word[2]\n",
    "\n",
    "# Sentiments\n",
    "sentiments = [AFINN.get(word, 0) for word in vocab]\n",
    "\n",
    "# Putting them all together\n",
    "data_for_export = np.asarray([words, scores_1, scores_2, scores_3, sentiments]).T\n",
    "\n",
    "# Save csv\n",
    "np.savetxt(\"data_export.csv\", np.array(data_for_export), delimiter=\",\", fmt = \"%s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00075942880165338011, 8.4287325377733651e-07, 0.0005908541508979128, 0.0020237386823193848, 0.002276600658452586, 0.0023608879838303196, 0.003119473912229922, 0.00050656682552017919, 0.0037937725152517913, 0.0041309218167627261, 8.4287325377733651e-07, 8.4287325377733651e-07, 0.0013494400792975157, 0.0045523584436513937, 0.002276600658452586, 0.0010965781031643147, 0.0023608879838303196, 0.0039623471660072589, 0.0021923133330748524, 0.0011808654285420483, 0.0010965781031643147, 8.4287325377733651e-07, 0.0020237386823193848, 8.4287325377733651e-07, 8.4287325377733651e-07, 0.0020237386823193848, 8.4287325377733651e-07, 0.002698037285341254, 0.0005908541508979128, 0.0012651527539197819, 0.0011808654285420483, 0.0037094851898740577, 0.00084371612703111382, 0.0013494400792975157, 8.4287325377733651e-07, 0.0064066796019615346, 0.0014337274046752496, 0.0079238514587607403, 0.0011808654285420483, 0.0011808654285420483, 0.0021923133330748524, 8.4287325377733651e-07, 0.0018551640315639178, 0.0089352993632935435]\n",
      "[array([ 0.02658001]), array([  3.96150429e-05]), array([ 0.01713477]), array([ 0.08499702]), array([ 0.10472363]), array([ 0.06610486]), array([ 0.11542053]), array([ 0.01114447]), array([ 0.21245126]), array([ 0.57419813]), array([  5.39438882e-05]), array([  4.88866487e-05]), array([ 0.09850913]), array([ 0.26403679]), array([ 0.28685168]), array([ 0.03838023]), array([ 0.07318753]), array([ 0.2100044]), array([ 0.17100044]), array([ 0.02952164]), array([ 0.03289734]), array([  2.19147046e-05]), array([ 0.04856973]), array([  2.36004511e-05]), array([  2.52861976e-05]), array([ 0.04856973]), array([ 0.00036244]), array([ 0.36963111]), array([ 0.01477135]), array([ 0.07590917]), array([ 0.0271599]), array([ 0.54158484]), array([ 0.02531148]), array([ 0.14573953]), array([  3.62435499e-05]), array([ 0.94818858]), array([ 0.06308401]), array([ 0.74484204]), array([ 0.04133029]), array([ 0.02361731]), array([ 0.26965454]), array([  8.51301986e-05]), array([ 0.16881993]), array([ 2.34998373])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3a6049adc4cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mcounts_red_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mweighted_counts_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_red_1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcounts_red_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mweighted_counts_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_red_2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcounts_red_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mweighted_counts_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores_red_3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcounts_red_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt\n",
    "\n",
    "word_counts = np.sum(DT_matrix_reduced, axis = 0) # this is the count of the words over all documents\n",
    "\n",
    "# Sentiments\n",
    "sent_array = [sent for sent in sentiments if sent != 0]\n",
    "sent_array = np.array(sent_array).reshape((len(sent_array),1))\n",
    "\n",
    "# Getting the weighted counts\n",
    "scores_red_1 = [w for (s,w) in zip(sent_array, scores_1) if s != 0]\n",
    "counts_red_1 = [w[0] for (s,w) in zip(sent_array, weighted_counts_1) if s != 0]\n",
    "\n",
    "scores_red_2 = [w for (s,w) in zip(sent_array, scores_2) if s != 0]\n",
    "counts_red_2 = [w[0] for (s,w) in zip(sent_array, weighted_counts_2) if s != 0]\n",
    "\n",
    "scores_red_3 = [w for (s,w) in zip(sent_array, scores_3) if s != 0]\n",
    "counts_red_3 = [w[0] for (s,w) in zip(sent_array, weighted_counts_3) if s != 0]\n",
    "\n",
    "print scores_red_1\n",
    "print counts_red_1\n",
    "\n",
    "weighted_counts_1 = scores_red_1 * counts_red_1\n",
    "weighted_counts_2 = scores_red_2 * counts_red_2\n",
    "weighted_counts_3 = scores_red_3 * counts_red_3\n",
    "\n",
    "weighted_counts_1 = weighted_counts_1.reshape((len(weighted_counts_1),1))\n",
    "weighted_counts_2 = weighted_counts_2.reshape((len(weighted_counts_2),1))\n",
    "weighted_counts_3 = weighted_counts_3.reshape((len(weighted_counts_3),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients 1: \n",
      "[[-0.16601521]]\n",
      "Coefficients 2: \n",
      "[[ 0.16633502]]\n",
      "Coefficients 3: \n",
      "[[ 0.29244862]]\n",
      "Y inter 1: \n",
      "[ 0.39693347]\n",
      "Y inter 2: \n",
      "[ 0.40132329]\n",
      "Y inter 3: \n",
      "[ 0.5000889]\n",
      "[array([-2]), array([-2]), array([-2]), array([-2]), array([-2]), array([-2]), array([-2]), array([-2]), array([-2]), array([-1]), array([-1]), array([-1]), array([-1]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([1]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([2]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([3]), array([4]), array([4]), array([5])]\n",
      "[array([  1.77003383e-05]), array([  2.19147046e-05]), array([  3.62435499e-05]), array([ 0.04083215]), array([ 0.05270065]), array([ 0.08010499]), array([ 0.17536147]), array([ 0.33990044]), array([ 10.36361215]), array([ 0.02193156]), array([ 0.04891109]), array([ 0.05270065]), array([ 0.08972892]), array([  1.68574651e-05]), array([  1.77003383e-05]), array([  1.77003383e-05]), array([  1.85432116e-05]), array([  1.85432116e-05]), array([  1.85432116e-05]), array([  1.85432116e-05]), array([  1.85432116e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  1.93860848e-05]), array([  2.02289581e-05]), array([  2.19147046e-05]), array([  2.19147046e-05]), array([  2.27575779e-05]), array([  2.27575779e-05]), array([  2.27575779e-05]), array([  2.36004511e-05]), array([  2.36004511e-05]), array([  2.36004511e-05]), array([  2.36004511e-05]), array([  2.36004511e-05]), array([  2.44433244e-05]), array([  2.52861976e-05]), array([  2.52861976e-05]), array([  2.61290709e-05]), array([  2.61290709e-05]), array([  2.61290709e-05]), array([  2.78148174e-05]), array([  2.78148174e-05]), array([  2.78148174e-05]), array([  2.86576906e-05]), array([  2.95005639e-05]), array([  3.28720569e-05]), array([  3.37149302e-05]), array([  3.37149302e-05]), array([  3.37149302e-05]), array([  3.45578034e-05]), array([  3.54006767e-05]), array([  3.54006767e-05]), array([  3.62435499e-05]), array([  3.70864232e-05]), array([  3.70864232e-05]), array([  3.70864232e-05]), array([  3.79292964e-05]), array([  3.96150429e-05]), array([  4.04579162e-05]), array([  4.21436627e-05]), array([  4.21436627e-05]), array([  4.38294092e-05]), array([  4.38294092e-05]), array([  4.46722825e-05]), array([  4.46722825e-05]), array([  4.63580290e-05]), array([  4.63580290e-05]), array([  4.88866487e-05]), array([  5.14152685e-05]), array([  5.14152685e-05]), array([  5.31010150e-05]), array([  5.39438882e-05]), array([  5.64725080e-05]), array([  6.06868743e-05]), array([  6.49012405e-05]), array([  7.08013533e-05]), array([  7.16442266e-05]), array([  7.50157196e-05]), array([  7.67014661e-05]), array([  8.51301986e-05]), array([  8.93445649e-05]), array([ 0.000118]), array([ 0.00012475]), array([ 0.00013149]), array([ 0.00013233]), array([ 0.00016436]), array([ 0.00023516]), array([ 0.00024022]), array([ 0.00372719]), array([ 0.00491311]), array([ 0.00583521]), array([ 0.00608892]), array([ 0.00675984]), array([ 0.00743583]), array([ 0.00777382]), array([ 0.00887967]), array([ 0.01114447]), array([ 0.01318169]), array([ 0.01344636]), array([ 0.01358965]), array([ 0.01477135]), array([ 0.01552825]), array([ 0.01687432]), array([ 0.01711117]), array([ 0.01713477]), array([ 0.0210929]), array([ 0.02126401]), array([ 0.02134408]), array([ 0.02160453]), array([ 0.02160453]), array([ 0.02193662]), array([ 0.0222704]), array([ 0.02227208]), array([ 0.02278034]), array([ 0.02354229]), array([ 0.02361731]), array([ 0.02530306]), array([ 0.02530306]), array([ 0.02531148]), array([ 0.02658001]), array([ 0.0269121]), array([ 0.0271599]), array([ 0.0271599]), array([ 0.02733944]), array([ 0.02741445]), array([ 0.02833824]), array([ 0.02834077]), array([ 0.02834077]), array([ 0.02952164]), array([ 0.02960761]), array([ 0.02961772]), array([ 0.0307025]), array([ 0.03103712]), array([ 0.031542]), array([ 0.03191371]), array([ 0.03238656]), array([ 0.03249698]), array([ 0.03289734]), array([ 0.03297573]), array([ 0.03306423]), array([ 0.03341487]), array([ 0.03373179]), array([ 0.03399392]), array([ 0.0342451]), array([ 0.03508544]), array([ 0.03543608]), array([ 0.03643235]), array([ 0.03643235]), array([ 0.03643488]), array([ 0.03643488]), array([ 0.03718841]), array([ 0.03718841]), array([ 0.03718841]), array([ 0.03795458]), array([ 0.03838023]), array([ 0.03840552]), array([ 0.03846705]), array([ 0.03896856]), array([ 0.03921974]), array([ 0.03946838]), array([ 0.0394903]), array([ 0.04073016]), array([ 0.04081361]), array([ 0.04081361]), array([ 0.0409864]), array([ 0.04133029]), array([ 0.04175004]), array([ 0.04402243]), array([ 0.04453152]), array([ 0.04460738]), array([ 0.04460738]), array([ 0.04460738]), array([ 0.0455455]), array([ 0.04587928]), array([ 0.04587928]), array([ 0.04604279]), array([ 0.04681065]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.04856973]), array([ 0.05009449]), array([ 0.05127367]), array([ 0.05263912]), array([ 0.05270065]), array([ 0.05313642]), array([ 0.05379976]), array([ 0.05430464]), array([ 0.05440157]), array([ 0.05550068]), array([ 0.05608057]), array([ 0.05666805]), array([ 0.05666805]), array([ 0.05700015]), array([ 0.05700015]), array([ 0.05734404]), array([ 0.05753621]), array([ 0.05819703]), array([ 0.05936525]), array([ 0.06012299]), array([ 0.06021655]), array([ 0.06072059]), array([ 0.06146822]), array([ 0.06146822]), array([ 0.06146822]), array([ 0.06258587]), array([ 0.06308401]), array([ 0.06357709]), array([ 0.06610486]), array([ 0.06610486]), array([ 0.06610486]), array([ 0.06678338]), array([ 0.07049623]), array([ 0.07091008]), array([ 0.07091008]), array([ 0.07286471]), array([ 0.07318753]), array([ 0.0742192]), array([ 0.07588388]), array([ 0.07588388]), array([ 0.07588894]), array([ 0.0761477]), array([ 0.07824561]), array([ 0.08300784]), array([ 0.08339641]), array([ 0.08423422]), array([ 0.08499702]), array([ 0.08633719]), array([ 0.08633719]), array([ 0.08633719]), array([ 0.08702076]), array([ 0.08972892]), array([ 0.09037203]), array([ 0.09181671]), array([ 0.09181671]), array([ 0.09181671]), array([ 0.0944313]), array([ 0.09459904]), array([ 0.09613812]), array([ 0.09646179]), array([ 0.09746481]), array([ 0.09746481]), array([ 0.09780701]), array([ 0.09850913]), array([ 0.09857066]), array([ 0.10328147]), array([ 0.10472363]), array([ 0.10876689]), array([ 0.10962831]), array([ 0.11331757]), array([ 0.11542053]), array([ 0.11542053]), array([ 0.11542053]), array([ 0.12789843]), array([ 0.12807375]), array([ 0.12818079]), array([ 0.1335853]), array([ 0.13382214]), array([ 0.13489344]), array([ 0.1369315]), array([ 0.13776173]), array([ 0.13852875]), array([ 0.13961858]), array([ 0.14030805]), array([ 0.14164316]), array([ 0.14172155]), array([ 0.1435194]), array([ 0.14416926]), array([ 0.14480647]), array([ 0.14573953]), array([ 0.14975666]), array([ 0.15049586]), array([ 0.15583715]), array([ 0.1569843]), array([ 0.15785162]), array([ 0.16321735]), array([ 0.16627192]), array([ 0.16881993]), array([ 0.17071976]), array([ 0.17100044]), array([ 0.17465092]), array([ 0.17772994]), array([ 0.18404896]), array([ 0.18623032]), array([ 0.18851029]), array([ 0.19011428]), array([ 0.19123446]), array([ 0.19340738]), array([ 0.2100044]), array([ 0.21245126]), array([ 0.21246306]), array([ 0.23201266]), array([ 0.23396054]), array([ 0.23776949]), array([ 0.24372439]), array([ 0.24431777]), array([ 0.25089134]), array([ 0.25501552]), array([ 0.26403679]), array([ 0.27139423]), array([ 0.27389756]), array([ 0.27389756]), array([ 0.2745137]), array([ 0.28513475]), array([ 0.28685168]), array([ 0.28689973]), array([ 0.28925134]), array([ 0.29236576]), array([ 0.30013789]), array([ 0.30500802]), array([ 0.30506449]), array([ 0.30829691]), array([ 0.31723305]), array([ 0.31968412]), array([ 0.32491247]), array([ 0.34529483]), array([ 0.35508396]), array([ 0.35616874]), array([ 0.36453594]), array([ 0.37842228]), array([ 0.38567773]), array([ 0.38872219]), array([ 0.39728999]), array([ 0.39892433]), array([ 0.40069942]), array([ 0.41819578]), array([ 0.45370013]), array([ 0.4669029]), array([ 0.47299097]), array([ 0.50748219]), array([ 0.51503686]), array([ 0.54063661]), array([ 0.54158484]), array([ 0.57419813]), array([ 0.59480217]), array([ 0.60904757]), array([ 0.63202598]), array([ 0.63427392]), array([ 0.64202667]), array([ 0.67270136]), array([ 0.67948649]), array([ 0.68925623]), array([ 0.70418773]), array([ 0.74484204]), array([ 0.80115777]), array([ 0.82848204]), array([ 0.90909191]), array([ 0.91284775]), array([ 0.94818858]), array([ 0.95342704]), array([ 0.96987487]), array([ 0.97716488]), array([ 0.99740226]), array([ 1.00615213]), array([ 1.09010652]), array([ 1.0958448]), array([ 1.25435218]), array([ 1.35115701]), array([ 1.47280975]), array([ 1.50705991]), array([ 1.68326509]), array([ 1.73581149]), array([ 1.91657493]), array([ 1.93019155]), array([ 1.93613381]), array([ 2.06035478]), array([ 2.11509434]), array([ 2.34998373]), array([ 2.40822965]), array([ 2.8251392]), array([ 3.10248074]), array([ 3.16866231]), array([ 6.36433618]), array([ 10.46335019]), array([ 10.95108132]), array([ 11.31305577]), array([ 25.55307404]), array([  1.77003383e-05]), array([  9.18731847e-05]), array([ 0.02599758]), array([ 0.0261552]), array([ 0.04014437]), array([ 0.07590917]), array([ 0.08533586]), array([ 0.24553235]), array([ 0.00011463]), array([ 0.0001711]), array([ 0.00035906]), array([ 0.02050458]), array([ 0.03845525]), array([ 0.04014437]), array([ 0.04856973]), array([ 0.05127872]), array([ 0.07091008]), array([ 0.08458992]), array([ 0.12520461]), array([ 0.19771025]), array([ 0.36963111]), array([  2.27575779e-05]), array([  2.44433244e-05]), array([ 0.0001121]), array([ 0.00036244]), array([ 0.02449221]), array([ 0.03758288]), array([ 0.04014437]), array([ 0.00010536]), array([ 0.26965454]), array([ 0.01446118])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt\n",
    "\n",
    "word_counts = np.sum(DT_matrix_reduced, axis = 0) # this is the count of the words over all documents\n",
    "\n",
    "# Sentiments\n",
    "sent_array = [sent for sent in sentiments if sent != 0]\n",
    "sent_array = np.array(sent_array).reshape((len(sent_array),1))\n",
    "\n",
    "# Getting the weighted counts\n",
    "scores_red_1 = [w[0] for (s,w) in zip(sent_array,scores_1) if s != 0]\n",
    "counts_red_1 = [w[0] for (s,w) in zip(sent_array,weighted_counts_1) if s != 0]\n",
    "\n",
    "scores_red_2 = [w[0] for (s,w) in zip(sent_array,scores_2) if s != 0]\n",
    "counts_red_2 = [w[0] for (s,w) in zip(sent_array,weighted_counts_2) if s != 0]\n",
    "\n",
    "scores_red_3 = [w[0] for (s,w) in zip(sent_array,scores_3) if s != 0]\n",
    "counts_red_3 = [w[0] for (s,w) in zip(sent_array,weighted_counts_3) if s != 0]\n",
    "\n",
    "weighted_counts_1 = scores_red_1 * counts_red_1\n",
    "weighted_counts_2 = scores_red_2 * counts_red_2\n",
    "weighted_counts_3 = scores_red_3 * counts_red_3\n",
    "\n",
    "weighted_counts_1 = weighted_counts_1.reshape((len(weighted_counts_1),1))\n",
    "weighted_counts_2 = weighted_counts_2.reshape((len(weighted_counts_2),1))\n",
    "weighted_counts_3 = weighted_counts_3.reshape((len(weighted_counts_3),1))\n",
    "\n",
    "# Regression\n",
    "regr_1 = linear_model.LinearRegression()\n",
    "regr_2 = linear_model.LinearRegression()\n",
    "regr_3 = linear_model.LinearRegression()\n",
    "\n",
    "regr_1.fit(sent_array, weighted_counts_1)\n",
    "regr_2.fit(sent_array, weighted_counts_2)\n",
    "regr_3.fit(sent_array, weighted_counts_3)\n",
    "\n",
    "# The coefficients\n",
    "print 'Coefficients 1: \\n', regr_1.coef_\n",
    "print 'Coefficients 2: \\n', regr_2.coef_\n",
    "print 'Coefficients 3: \\n', regr_3.coef_\n",
    "\n",
    "print 'Y inter 1: \\n', regr_1.intercept_\n",
    "print 'Y inter 2: \\n', regr_2.intercept_\n",
    "print 'Y inter 3: \\n', regr_3.intercept_\n",
    "\n",
    "# Plot outputs\n",
    "x = sorted(sent_array)\n",
    "y = [w for (s, w) in sorted(zip(sent_array, weighted_counts_1))]\n",
    "y_fit_1 = [regr_1.intercept_[0] + regr_1.coef_[0] * y_val for y_val in y]\n",
    "\n",
    "\n",
    "print x\n",
    "print y\n",
    "\n",
    "#plt.plot(x, y, 'x')\n",
    "plt.plot(x, y_fit_1, '-')\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
