{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup # for maniuplating html\n",
    "\n",
    "# START OF REVIEW EXTRACTING\n",
    "\n",
    "with open('car_output.txt', 'r') as stringfile: # so we load in ALL the data as a massive string\n",
    "\tcar_output = stringfile.read()\n",
    "\n",
    "out = [\"<!DOCTYPE html>\" + rest for rest in car_output.split(\"<!DOCTYPE html>\")] # split up by start pf page, but add the start point\n",
    "\n",
    "out_soup = [BeautifulSoup(html, \"html.parser\") for html in out[1:]] # convert to beautiful soup\n",
    "\n",
    "# I think the above is standard, now we actually do our document (i.e. if change URLs to look at)\n",
    "\n",
    "# Now extracting reviews - golf gives me 2098, could add another car to get more\n",
    "review_rating = []\n",
    "for page in out_soup:\n",
    "\tpage_contents = page.find_all(\"div\", itemprop=\"review\") # locating all reviews on a page\n",
    "\tresult = [(review.find(itemprop=\"reviewBody\").get_text().encode(\"ascii\",\"replace\"), # getting text\n",
    "\t           review.find(class_=\"size8\").get_text().encode(\"ascii\",\"replace\").split()[0]) for review in page_contents] # getting rating\n",
    "\t[review_rating.append(review) for review in result]\n",
    "\n",
    "# Removing empty reviews and getting rid of paragraphs and saving review and rating as a tuple\n",
    "review_rating_red = [(review[0].replace('\\n', ' ').replace('\\r', ''), review[1]) for review in review_rating if review[0] != \"Reviewer left no comment\"] # leaves us with 151 reviews\n",
    "\n",
    "# Save all data just for safety\n",
    "with open(\"reviews_ratings.txt\", \"w\") as out_file:\n",
    "\tout_file.write(\"\\n\".join(\"%s %s\" % review for review in review_rating_red))\n",
    "    \n",
    "# Getting reviews\n",
    "reviews = [review[0] for review in review_rating_red]\n",
    "reviews[1] = \"Top car I've had every convertible VW have made and still have a mk 1 golf but not cabriollet\" # Just randomly noticed this review was written by an idiot\n",
    "\n",
    "# Getting ratings\n",
    "ratings = [review[1] for review in review_rating_red] # distribution of ratings is not high, may not use\n",
    "\n",
    "# END OF REVIEW EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer # could change the stemmer or lemmatise\n",
    "\n",
    "# START OF CLASS CREATION\n",
    "# Will create a class that will get rating and text\n",
    "class Review():\n",
    "    # On initialising\n",
    "    def __init__(self, review_text, review_rating):\n",
    "        self.text = review_text.lower()\n",
    "        self.rating = review_rating\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text)) # Would like to split on punct as well to remove later\n",
    "    \n",
    "    # Remove non-alphanumeric\n",
    "    def token_remove_alpha(self):\n",
    "            self.tokens = np.array([word for word in self.tokens if word.isalpha()])\n",
    "    \n",
    "    # Remove stop words\n",
    "    def token_stop_remove(self, stopwords):\n",
    "            self.tokens = np.array([word for word in self.tokens if word not in stopwords])\n",
    "            \n",
    "    # Now stem everything\n",
    "    def token_stem(self):\n",
    "            self.tokens = np.array([PorterStemmer().stem(word) for word in self.tokens])\n",
    "    \n",
    "    # Define a process that will clean our tokens\n",
    "    def token_clean_up(self, stopwords):    \n",
    "        # Now cleaning up\n",
    "        self.token_remove_alpha()\n",
    "        self.token_stop_remove(stopwords)\n",
    "        self.token_stem()\n",
    "    \n",
    "# END OF CLASS CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the stop words used, also excluded some common \"mistakes\" like youll or youre\n",
    "\n",
    "Also included some words previously included like \"high\", \"first\", \"long\", as they could refer to performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# START OF CORPUS\n",
    "class Review_set():\n",
    "    # On initialising\n",
    "    def __init__(self, review_data, stopword_file):\n",
    "        self.reviews = [Review(review[0], review[1]) for review in review_data] # feed in the tuples of ratings etc.\n",
    "        self.create_stopwords(stopword_file) # stopword element\n",
    "        \n",
    "        # Now for each document we will get a cleaned up token set - note that this is done for each document\n",
    "        self.clean_reviews() \n",
    "        \n",
    "        self.tokenise_reviews() # return reviews but in tokenised form\n",
    "        \n",
    "        # Getting unique tokens in the review set - note we pull data from each document\n",
    "        self.get_unique_tokens()\n",
    "    \n",
    "    # Loading and creating stopwords\n",
    "    def create_stopwords(self, stopword_file):\n",
    "        self.stopwords = np.array(np.loadtxt(\"stopwords.txt\", dtype = str, delimiter = \"\\n\"))\n",
    "        \n",
    "    # Cleaning all documents\n",
    "    def clean_reviews(self):\n",
    "        [review.token_clean_up(self.stopwords) for review in self.reviews]\n",
    "        \n",
    "    # Getting reviews in tokenised form\n",
    "    def tokenise_reviews(self):\n",
    "        self.tokenised_reviews = [\" \".join(review.tokens) for review in self.reviews]\n",
    "        \n",
    "    # Getting the unique set of tokens\n",
    "    def get_unique_tokens(self):\n",
    "        self.unique_tokens = set()\n",
    "        for review in self.reviews:\n",
    "            self.unique_tokens = self.unique_tokens.union(review.tokens)\n",
    "        \n",
    "    # Document term matrix of CLEANED UP TOKENS - NOTE that will return in alphabetical order of words\n",
    "    def doc_term_mat(self):\n",
    "        vectorisor = CountVectorizer()\n",
    "        count_fit = vectorisor.fit_transform(self.tokenised_reviews)\n",
    "        \n",
    "        self.DTM = count_fit.toarray() # return array of counts\n",
    "    \n",
    "    # Get tf_idf scores\n",
    "    def tf_idf_score(self):   \n",
    "        vectorisor = TfidfVectorizer()\n",
    "        tf_idf_fit = vectorisor.fit_transform(self.tokenised_reviews)\n",
    "        \n",
    "        # Now return tuples of words and their associated score - note that am returning ALL\n",
    "        self.tf_idf = zip(vectorisor.get_feature_names(),vectorisor.idf_)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately will want to find common topics or \"trends\" in the reviews. To identify this could look for\n",
    "- Commonly occuring words: see common trends\n",
    "- Important words: maybe we can use this to identify more in depth reviews that focus on faults\n",
    "\n",
    "I guess then what would be left would be to try and identify the topics by looking at the common words\n",
    "\n",
    "Could extend and look at some kind of sentiment analysis of the top words in a topic\n",
    "\n",
    "Issues\n",
    "- Haven't really specified stop words\n",
    "- Superlatives etc. will really mess up the data, really want to focus on the technical aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the corpus etc\n",
    "review_data = zip(reviews, ratings)\n",
    "review_corpus = Review_set(review_data, \"stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to be careful because the actual \"REVIEWS\" does NOT have any of these attributes and need to go over it better\n",
    "#print review_corpus.reviews[1].text # THIS returns the text / tokens / rating etc\n",
    "\n",
    "#print review_corpus.stopwords # gets our stopwords\n",
    "\n",
    "#print review_corpus.unique_tokens # now correct (Y)\n",
    "\n",
    "\n",
    "#print review_corpus.tokenised_reviews # this correctly returns a list of all reivews in tokenised form\n",
    "\n",
    "# GOT EVERYTHING TO WORK\n",
    "#from operator import itemgetter #sorted(data,key=itemgetter(1))\n",
    "#review_corpus.tf_idf_score()\n",
    "\n",
    "#print sorted(review_corpus.tf_idf, key = itemgetter(1)) # now we can see some of the most common terms used, e.g. car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2098L, 2906L)\n",
      "1334\n",
      "(2098L, 1548L)\n",
      "12\n",
      "(2086L, 1548L)\n"
     ]
    }
   ],
   "source": [
    "review_corpus.doc_term_mat()\n",
    "print review_corpus.DTM.shape # the reviews are far too dissimilar to each other, need to cut down on rare words\n",
    "\n",
    "print sum(np.sum(review_corpus.DTM, axis = 0) == 1) # 1334 words only have one entry\n",
    "\n",
    "vectorisor = CountVectorizer(min_df = 0.0004767)\n",
    "count_fit = vectorisor.fit_transform(review_corpus.tokenised_reviews)\n",
    "\n",
    "print count_fit.toarray().shape\n",
    "print sum(np.sum(count_fit.toarray(), axis = 1) == 0)\n",
    "\n",
    "print count_fit.toarray()[np.sum(count_fit.toarray(), axis = 1) != 0,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA instance at 0x000000003430B308>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lda\n",
    "\n",
    "#print np.argmin(np.sum(review_corpus.DTM, axis = 1)) # PROBELM in that some review was only stop words\n",
    "#print review_corpus.reviews[137].text\n",
    "\n",
    "vectorisor = CountVectorizer(min_df = 0.0004767)\n",
    "count_fit = vectorisor.fit_transform(review_corpus.tokenised_reviews)\n",
    "DT_matrix_reduced = count_fit.toarray()[np.sum(count_fit.toarray(), axis = 1) != 0,]\n",
    "\n",
    "# PARAMETERS\n",
    "n_topics = 5\n",
    "n_iter = 30000\n",
    "###\n",
    "\n",
    "lda_model = lda.LDA(n_iter = n_iter, n_topics = n_topics)\n",
    "#lda_model.fit(review_corpus.DTM)\n",
    "lda_model.fit(DT_matrix_reduced)\n",
    "\n",
    "# Running an LDA to try and uncover topics present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print [key for key in vectorisor.vocabulary_.iterkeys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: vw problem servic issu cost expens year\n",
      "Topic 1: year love mile new reliabl old bought\n",
      "Topic 2: good great fuel drive economi reliabl engin\n",
      "Topic 3: great drive reliabl good comfort love fuel\n",
      "Topic 4: best own drive vw car love year\n"
     ]
    }
   ],
   "source": [
    "# Note, maybe should take names from an attribute of DT_MATRIX_REDUCE\n",
    "\n",
    "# Vocab needs to be a sorted list of words that went IN to the LDA\n",
    "# vocab = sorted(list(review_corpus.unique_tokens))\n",
    "vocab = sorted([key for key in vectorisor.vocabulary_.iterkeys()]) # nice\n",
    "topic_word = lda_model.topic_word_\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
