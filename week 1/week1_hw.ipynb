{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 HW\n",
    "\n",
    "\n",
    "1. This assignment is a group effort.\n",
    "2. Submission to be uploaded into your group repositories in the folder week1\n",
    "3. Deadline is 27th of April 5:00 PM.\n",
    "4. Please follow google's [python styleguide](https://google.github.io/styleguide/pyguide.html) for your code. Pay attention to the guidelines for naming convention, comments and main.\n",
    "5. Code will be checked for plagiarism. Compelling signs of a duplicated effort will lead to a rejection of submission and will attract a 100\\% grade penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the template provided as a starting point. Extend the classes as you see fit. Be careful to place new attributes and methods in the approriate class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        \"\"\"\n",
    "        The __init__ method is called everytime an object is instantiated.\n",
    "        This is where you will define all the properties of the object that it must have\n",
    "        when it is `born`.\n",
    "        \"\"\"\n",
    "        \n",
    "        #These are data members\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n",
    "    def demo_self():\n",
    "        print 'this will error out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "        \n",
    "        \n",
    "    ###### 1.1 doc_term ######   \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description: create a D by V array of frequency counts.\n",
    "        \"\"\"\n",
    "        def has_term(doc):\n",
    "            \n",
    "            #initialize an array with size V\n",
    "            has_term = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                has_term[list(self.token_set).index(token)] += (token in self.token_set)\n",
    "            return has_term\n",
    "                   \n",
    "        self.doc_term_matrix = [[has_term(doc)] for doc in self.docs]\n",
    "        \n",
    "        \n",
    "   \n",
    "    ###### 1.2 tf_idf ######\n",
    "    \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description: create a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        def tf(doc):\n",
    "            #initialize an array of size V\n",
    "            term_count = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                if token in self.token_set:\n",
    "                    term_count[list(self.token_set).index(token)] += 1 \n",
    "            return term_count\n",
    "        \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            freq = 0\n",
    "            for doc in self.docs:\n",
    "                freq += (token in doc.tokens)\n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/freq)\n",
    "            \n",
    "        def get_score(doc):\n",
    "            tf = tf(doc)\n",
    "        \n",
    "            for i,term in enumerate(tf):\n",
    "                if term != 0:\n",
    "                    tf[i] = (1 + math.log(term)) * idf[i]\n",
    "            return tf\n",
    "            \n",
    "        self.tf_idf = [get_score(doc) for doc in self.docs]\n",
    " \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "Extend the classes to include the following methods\n",
    "\n",
    "1. document_term_matrix - which returns a D by V array of frequency counts.\n",
    "2. tf_idf - returns a D by V array of tf-idf scores\n",
    "3. dict_rank - returns the top `n` documents based on a given dictionary and represenation of tokens (eg. doc-term matrix or tf-idf matrix)  \n",
    "\n",
    "Include subroutines as and when necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "Pick a dictionary (or dictionaries) of your choice from the Harvard IV set, the Loughran-McDonald set, or some other of your choosing that you think may be relevant for the data you collected. Then conduct the following exercise:\n",
    "1. Use the two methods above to score each document in your data.\n",
    "2. Explore whether the scores diﬀer according to the meta data ﬁelds you gathered: for example, do diﬀerent speakers/sources/etc tend to receive a higher score than others?\n",
    "3. Do the answers to the previous question depend on whether tf-idf weighting is applied or not? Why do you think there is (or is not) a diﬀerence in your answers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "We will now do a sentiment analysis using the AFINN list of words. AFINN is a list of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. A positive valence score can be interpreted as the word conveying a postive emotion and vice versa. \n",
    "\n",
    "Load _AFINN-111.txt_ from ./data/AFINN. Inspect the contents of the file and write a function that converts it into a dictionary where the keys are words and values are the valence scores attributed to them. You may use the readme file for hints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "Now, use the presedential speeches from last week's HW to calculate its sentiment score. Match every word against the dictionary and come up with a metric that captures the sentiment value. If a word is not present mark its score as 0. Write a function that takes in a list of word and returns their sentiment score. What is the score of the speech you have been assigned? Which year, president gave the least and most positive speech?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
