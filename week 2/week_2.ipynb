{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        \"\"\"\n",
    "        The __init__ method is called everytime an object is instantiated.\n",
    "        This is where you will define all the properties of the object that it must have\n",
    "        when it is `born`.\n",
    "        \"\"\"\n",
    "        \n",
    "        #These are data members\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n",
    "    def demo_self():\n",
    "        print 'this will error out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "        #create document_term_matrix\n",
    "        #self.document_term_matrix()\n",
    "        \n",
    "        #create tf_idf scores\n",
    "        #self.tf_idf()\n",
    "        \n",
    "        #create dict_rank\n",
    "        #self.dict_rank(rep_tokens)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "        \n",
    "        \n",
    "    ###### 1.1 doc_term ######   \n",
    "    def document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description: create a D by V array of frequency counts.\n",
    "        \"\"\"\n",
    "        def term_count(doc):\n",
    "            \n",
    "            #initialize an array with size V\n",
    "            term_count = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                if token in self.token_set:\n",
    "                    term_count[list(self.token_set).index(token)] += 1 \n",
    "            return term_count\n",
    "                   \n",
    "        self.doc_term_matrix = [term_count(doc) for doc in self.docs]\n",
    "        \n",
    "        \n",
    "   \n",
    "    ###### 1.2 tf_idf ######\n",
    "    \n",
    "    def tf_idf(self):\n",
    "        \"\"\"\n",
    "        description: create a D by V array of tf-idf scores\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        def tf(doc):\n",
    "            #initialize an array of size V\n",
    "            term_count = [0]*len(self.token_set)\n",
    "            for token in doc.tokens:\n",
    "                if token in self.token_set:\n",
    "                    term_count[list(self.token_set).index(token)] += 1 \n",
    "            return term_count\n",
    "        \n",
    "        idf = [0]*len(self.token_set)\n",
    "        for token in self.token_set:\n",
    "            freq = 0\n",
    "            for doc in self.docs:\n",
    "                freq += (token in doc.tokens)\n",
    "            idf[list(self.token_set).index(token)] = math.log(self.N/freq)\n",
    "            \n",
    "        def get_score(doc):\n",
    "            t_f = tf(doc)\n",
    "        \n",
    "            for i,term in enumerate(t_f):\n",
    "                if term != 0:\n",
    "                    t_f[i] = (1 + math.log(term)) * idf[i]\n",
    "            return t_f\n",
    "            \n",
    "        self.tf_idf = [(doc, get_score(doc)) for doc in self.docs]    \n",
    "        \n",
    "    \n",
    "    ###### 1.3 dict_rank ######\n",
    "    def dict_rank(self, n, dictionary, rep_tokens):\n",
    "        \"\"\"\n",
    "        return top N documents given a representation of tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        if rep_tokens == \"doc_term\":\n",
    "            self.document_term_matrix()\n",
    "            scores = self.doc_term_matrix\n",
    "        elif rep_tokens == \"tf_idf\":\n",
    "            self.tf_idf()\n",
    "            scores = self.tf_idf\n",
    "        else: print(\"dude, what's wrong?\")\n",
    "\n",
    "        weights = [0] * self.N\n",
    "        \n",
    "        for i, doc in enumerate(self.docs):\n",
    "            for j, token in enumerate(self.token_set):\n",
    "                if token in dictionary:\n",
    "                    weights[i] += scores[i][j]\n",
    "                    \n",
    "        which_max = heapq.nlargest(n, range(len(weights)), weights.__getitem__)             \n",
    "        self.dict_ranking = [self.docs[i] for i in which_max]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "def parse_text(textraw, regex): # So this is a necessary process to perform in order to convert into our class\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)\n",
    "\n",
    "text = open('./data/pres_speech/sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)[1:10]\n",
    "\n",
    "corpus = Corpus(pres_speech_list, './data/stopwords/stopwords.txt', 2)\n",
    "\n",
    "corpus.tf_idf()\n",
    "tfidf = corpus.tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################# WEEK 2 ###################\n",
    "\n",
    "pres_list = [pair[0].pres for pair in tfidf]\n",
    "scores = [pair[1] for pair in tfidf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.64433277201665784, 0.6443327720166564, 1.1708700837467513e-15)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(scores)\n",
    "P, D, Q = np.linalg.svd(X, full_matrices=False)\n",
    "X_hat = np.dot(np.dot(P, np.diag(D)), Q)\n",
    "\n",
    "# we need to do the dot product for a \"reasonable\" number of sv's\n",
    "\n",
    "print(np.std(X), np.std(X_hat), np.std(X - X_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(doc1, doc2):\n",
    "    return np.dot(doc1, doc2) / (math.sqrt(np.dot(doc1, doc1)) * math.sqrt(np.dot(doc2, doc2)))\n",
    "\n",
    "def cos_sim_map(X):\n",
    "    return map(lambda doc1: map(lambda doc2: cosine_sim(doc1, doc2), X), X)\n",
    "\n",
    "sim_X = cos_sim_map(X)\n",
    "sim_X_hat = cos_sim_map(X_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0,\n",
       "  0.11761926595678301,\n",
       "  0.085658410521250658,\n",
       "  0.035057229723018953,\n",
       "  0.058110020179341554,\n",
       "  0.053599987817413153,\n",
       "  0.068531075901293551,\n",
       "  0.020930661939335817,\n",
       "  0.047803809189043367],\n",
       " [0.11761926595678301,\n",
       "  1.0,\n",
       "  0.11278344021576492,\n",
       "  0.083696340130119257,\n",
       "  0.090660185946278463,\n",
       "  0.10384023999128583,\n",
       "  0.080855728913674207,\n",
       "  0.074985617591664916,\n",
       "  0.063442749735928713],\n",
       " [0.085658410521250658,\n",
       "  0.11278344021576492,\n",
       "  1.0000000000000002,\n",
       "  0.09933097933469659,\n",
       "  0.091472580073926585,\n",
       "  0.070834577212945296,\n",
       "  0.06506419910863484,\n",
       "  0.050515651088784905,\n",
       "  0.070498535579454527],\n",
       " [0.035057229723018953,\n",
       "  0.083696340130119257,\n",
       "  0.09933097933469659,\n",
       "  0.99999999999999989,\n",
       "  0.1293534593913879,\n",
       "  0.063965286106769284,\n",
       "  0.092040179590605231,\n",
       "  0.0789226761113965,\n",
       "  0.070451421053688151],\n",
       " [0.058110020179341554,\n",
       "  0.090660185946278463,\n",
       "  0.091472580073926585,\n",
       "  0.1293534593913879,\n",
       "  1.0000000000000002,\n",
       "  0.11364819094031955,\n",
       "  0.090941755263055812,\n",
       "  0.050296231958744969,\n",
       "  0.064317551274297294],\n",
       " [0.053599987817413153,\n",
       "  0.10384023999128583,\n",
       "  0.070834577212945296,\n",
       "  0.063965286106769284,\n",
       "  0.11364819094031955,\n",
       "  1.0000000000000002,\n",
       "  0.10895137447406701,\n",
       "  0.063778378724233614,\n",
       "  0.075247258369818423],\n",
       " [0.068531075901293551,\n",
       "  0.080855728913674207,\n",
       "  0.06506419910863484,\n",
       "  0.092040179590605231,\n",
       "  0.090941755263055812,\n",
       "  0.10895137447406701,\n",
       "  0.99999999999999989,\n",
       "  0.12348921487088063,\n",
       "  0.1326658991599137],\n",
       " [0.020930661939335817,\n",
       "  0.074985617591664916,\n",
       "  0.050515651088784905,\n",
       "  0.0789226761113965,\n",
       "  0.050296231958744969,\n",
       "  0.063778378724233614,\n",
       "  0.12348921487088063,\n",
       "  1.0,\n",
       "  0.14875975597926738],\n",
       " [0.047803809189043367,\n",
       "  0.063442749735928713,\n",
       "  0.070498535579454527,\n",
       "  0.070451421053688151,\n",
       "  0.064317551274297294,\n",
       "  0.075247258369818423,\n",
       "  0.1326658991599137,\n",
       "  0.14875975597926738,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.99999999999999978,\n",
       "  0.11761926595678299,\n",
       "  0.085658410521250714,\n",
       "  0.035057229723019029,\n",
       "  0.058110020179341547,\n",
       "  0.053599987817413403,\n",
       "  0.068531075901293453,\n",
       "  0.020930661939335942,\n",
       "  0.047803809189043617],\n",
       " [0.11761926595678299,\n",
       "  0.99999999999999989,\n",
       "  0.11278344021576388,\n",
       "  0.083696340130120214,\n",
       "  0.090660185946277214,\n",
       "  0.10384023999128687,\n",
       "  0.080855728913673264,\n",
       "  0.074985617591664652,\n",
       "  0.063442749735928297],\n",
       " [0.085658410521250714,\n",
       "  0.11278344021576388,\n",
       "  0.99999999999999989,\n",
       "  0.09933097933469634,\n",
       "  0.091472580073926474,\n",
       "  0.070834577212945893,\n",
       "  0.065064199108635243,\n",
       "  0.050515651088785293,\n",
       "  0.070498535579454957],\n",
       " [0.035057229723019029,\n",
       "  0.083696340130120214,\n",
       "  0.09933097933469634,\n",
       "  1.0,\n",
       "  0.1293534593913874,\n",
       "  0.063965286106770963,\n",
       "  0.092040179590604509,\n",
       "  0.078922676111396972,\n",
       "  0.070451421053688013],\n",
       " [0.058110020179341547,\n",
       "  0.090660185946277214,\n",
       "  0.091472580073926474,\n",
       "  0.1293534593913874,\n",
       "  0.99999999999999989,\n",
       "  0.11364819094031869,\n",
       "  0.090941755263056576,\n",
       "  0.050296231958744574,\n",
       "  0.064317551274296406],\n",
       " [0.053599987817413403,\n",
       "  0.10384023999128687,\n",
       "  0.070834577212945893,\n",
       "  0.063965286106770963,\n",
       "  0.11364819094031869,\n",
       "  1.0000000000000002,\n",
       "  0.10895137447406733,\n",
       "  0.063778378724233684,\n",
       "  0.075247258369819242],\n",
       " [0.068531075901293453,\n",
       "  0.080855728913673264,\n",
       "  0.065064199108635243,\n",
       "  0.092040179590604509,\n",
       "  0.090941755263056617,\n",
       "  0.10895137447406733,\n",
       "  1.0000000000000002,\n",
       "  0.12348921487088058,\n",
       "  0.13266589915991508],\n",
       " [0.020930661939335942,\n",
       "  0.074985617591664652,\n",
       "  0.050515651088785293,\n",
       "  0.078922676111396972,\n",
       "  0.050296231958744574,\n",
       "  0.063778378724233684,\n",
       "  0.12348921487088058,\n",
       "  1.0,\n",
       "  0.14875975597926686],\n",
       " [0.047803809189043617,\n",
       "  0.063442749735928297,\n",
       "  0.070498535579454957,\n",
       "  0.070451421053688013,\n",
       "  0.064317551274296406,\n",
       "  0.075247258369819242,\n",
       "  0.13266589915991508,\n",
       "  0.14875975597926686,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_X_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
